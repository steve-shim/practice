{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute every step manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 3: w = 1.872, loss = 0.76800019\n",
      "epoch 5: w = 1.980, loss = 0.01966083\n",
      "epoch 7: w = 1.997, loss = 0.00050332\n",
      "epoch 9: w = 1.999, loss = 0.00001288\n",
      "epoch 11: w = 2.000, loss = 0.00000033\n",
      "epoch 13: w = 2.000, loss = 0.00000001\n",
      "epoch 15: w = 2.000, loss = 0.00000000\n",
      "epoch 17: w = 2.000, loss = 0.00000000\n",
      "epoch 19: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Linear regression\n",
    "# f = w * x \n",
    "# here : f = 2 * x\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# model output\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean()\n",
    "\n",
    "# J = MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N * 2x(w*x - y)\n",
    "\n",
    "def gradient(x, y, y_pred):\n",
    "    return np.dot(2*x, y_pred - y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass   \n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # calculate gradients\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "    \n",
    "    # update weights\n",
    "    w -= learning_rate * dw\n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "     \n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we replace the manually computed gradient with autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "w.grad tensor(-30.)\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "w.grad tensor(-25.5000)\n",
      "epoch 2: w = 0.555, loss = 21.67499924\n",
      "w.grad tensor(-21.6750)\n",
      "epoch 3: w = 0.772, loss = 15.66018772\n",
      "w.grad tensor(-18.4238)\n",
      "epoch 4: w = 0.956, loss = 11.31448650\n",
      "w.grad tensor(-15.6602)\n",
      "epoch 5: w = 1.113, loss = 8.17471695\n",
      "w.grad tensor(-13.3112)\n",
      "epoch 6: w = 1.246, loss = 5.90623236\n",
      "w.grad tensor(-11.3145)\n",
      "epoch 7: w = 1.359, loss = 4.26725292\n",
      "w.grad tensor(-9.6173)\n",
      "epoch 8: w = 1.455, loss = 3.08308983\n",
      "w.grad tensor(-8.1747)\n",
      "epoch 9: w = 1.537, loss = 2.22753215\n",
      "w.grad tensor(-6.9485)\n",
      "epoch 10: w = 1.606, loss = 1.60939169\n",
      "w.grad tensor(-5.9062)\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "w.grad tensor(-5.0203)\n",
      "epoch 12: w = 1.716, loss = 0.84011245\n",
      "w.grad tensor(-4.2673)\n",
      "epoch 13: w = 1.758, loss = 0.60698116\n",
      "w.grad tensor(-3.6272)\n",
      "epoch 14: w = 1.794, loss = 0.43854395\n",
      "w.grad tensor(-3.0831)\n",
      "epoch 15: w = 1.825, loss = 0.31684780\n",
      "w.grad tensor(-2.6206)\n",
      "epoch 16: w = 1.851, loss = 0.22892261\n",
      "w.grad tensor(-2.2275)\n",
      "epoch 17: w = 1.874, loss = 0.16539653\n",
      "w.grad tensor(-1.8934)\n",
      "epoch 18: w = 1.893, loss = 0.11949898\n",
      "w.grad tensor(-1.6094)\n",
      "epoch 19: w = 1.909, loss = 0.08633806\n",
      "w.grad tensor(-1.3680)\n",
      "epoch 20: w = 1.922, loss = 0.06237914\n",
      "w.grad tensor(-1.1628)\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "w.grad tensor(-0.9884)\n",
      "epoch 22: w = 1.944, loss = 0.03256231\n",
      "w.grad tensor(-0.8401)\n",
      "epoch 23: w = 1.952, loss = 0.02352631\n",
      "w.grad tensor(-0.7141)\n",
      "epoch 24: w = 1.960, loss = 0.01699772\n",
      "w.grad tensor(-0.6070)\n",
      "epoch 25: w = 1.966, loss = 0.01228084\n",
      "w.grad tensor(-0.5159)\n",
      "epoch 26: w = 1.971, loss = 0.00887291\n",
      "w.grad tensor(-0.4385)\n",
      "epoch 27: w = 1.975, loss = 0.00641066\n",
      "w.grad tensor(-0.3728)\n",
      "epoch 28: w = 1.979, loss = 0.00463169\n",
      "w.grad tensor(-0.3168)\n",
      "epoch 29: w = 1.982, loss = 0.00334642\n",
      "w.grad tensor(-0.2693)\n",
      "epoch 30: w = 1.985, loss = 0.00241778\n",
      "w.grad tensor(-0.2289)\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "w.grad tensor(-0.1946)\n",
      "epoch 32: w = 1.989, loss = 0.00126211\n",
      "w.grad tensor(-0.1654)\n",
      "epoch 33: w = 1.991, loss = 0.00091188\n",
      "w.grad tensor(-0.1406)\n",
      "epoch 34: w = 1.992, loss = 0.00065882\n",
      "w.grad tensor(-0.1195)\n",
      "epoch 35: w = 1.993, loss = 0.00047601\n",
      "w.grad tensor(-0.1016)\n",
      "epoch 36: w = 1.994, loss = 0.00034392\n",
      "w.grad tensor(-0.0863)\n",
      "epoch 37: w = 1.995, loss = 0.00024848\n",
      "w.grad tensor(-0.0734)\n",
      "epoch 38: w = 1.996, loss = 0.00017952\n",
      "w.grad tensor(-0.0624)\n",
      "epoch 39: w = 1.996, loss = 0.00012971\n",
      "w.grad tensor(-0.0530)\n",
      "epoch 40: w = 1.997, loss = 0.00009371\n",
      "w.grad tensor(-0.0451)\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "w.grad tensor(-0.0383)\n",
      "epoch 42: w = 1.998, loss = 0.00004891\n",
      "w.grad tensor(-0.0326)\n",
      "epoch 43: w = 1.998, loss = 0.00003534\n",
      "w.grad tensor(-0.0277)\n",
      "epoch 44: w = 1.998, loss = 0.00002553\n",
      "w.grad tensor(-0.0235)\n",
      "epoch 45: w = 1.999, loss = 0.00001845\n",
      "w.grad tensor(-0.0200)\n",
      "epoch 46: w = 1.999, loss = 0.00001333\n",
      "w.grad tensor(-0.0170)\n",
      "epoch 47: w = 1.999, loss = 0.00000963\n",
      "w.grad tensor(-0.0144)\n",
      "epoch 48: w = 1.999, loss = 0.00000696\n",
      "w.grad tensor(-0.0123)\n",
      "epoch 49: w = 1.999, loss = 0.00000503\n",
      "w.grad tensor(-0.0104)\n",
      "epoch 50: w = 1.999, loss = 0.00000363\n",
      "w.grad tensor(-0.0089)\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "w.grad tensor(-0.0075)\n",
      "epoch 52: w = 2.000, loss = 0.00000190\n",
      "w.grad tensor(-0.0064)\n",
      "epoch 53: w = 2.000, loss = 0.00000137\n",
      "w.grad tensor(-0.0054)\n",
      "epoch 54: w = 2.000, loss = 0.00000099\n",
      "w.grad tensor(-0.0046)\n",
      "epoch 55: w = 2.000, loss = 0.00000071\n",
      "w.grad tensor(-0.0039)\n",
      "epoch 56: w = 2.000, loss = 0.00000052\n",
      "w.grad tensor(-0.0033)\n",
      "epoch 57: w = 2.000, loss = 0.00000037\n",
      "w.grad tensor(-0.0028)\n",
      "epoch 58: w = 2.000, loss = 0.00000027\n",
      "w.grad tensor(-0.0024)\n",
      "epoch 59: w = 2.000, loss = 0.00000019\n",
      "w.grad tensor(-0.0021)\n",
      "epoch 60: w = 2.000, loss = 0.00000014\n",
      "w.grad tensor(-0.0017)\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "w.grad tensor(-0.0015)\n",
      "epoch 62: w = 2.000, loss = 0.00000007\n",
      "w.grad tensor(-0.0013)\n",
      "epoch 63: w = 2.000, loss = 0.00000005\n",
      "w.grad tensor(-0.0011)\n",
      "epoch 64: w = 2.000, loss = 0.00000004\n",
      "w.grad tensor(-0.0009)\n",
      "epoch 65: w = 2.000, loss = 0.00000003\n",
      "w.grad tensor(-0.0008)\n",
      "epoch 66: w = 2.000, loss = 0.00000002\n",
      "w.grad tensor(-0.0007)\n",
      "epoch 67: w = 2.000, loss = 0.00000001\n",
      "w.grad tensor(-0.0006)\n",
      "epoch 68: w = 2.000, loss = 0.00000001\n",
      "w.grad tensor(-0.0005)\n",
      "epoch 69: w = 2.000, loss = 0.00000001\n",
      "w.grad tensor(-0.0004)\n",
      "epoch 70: w = 2.000, loss = 0.00000001\n",
      "w.grad tensor(-0.0003)\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-0.0003)\n",
      "epoch 72: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-0.0002)\n",
      "epoch 73: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-0.0002)\n",
      "epoch 74: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-0.0002)\n",
      "epoch 75: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-0.0002)\n",
      "epoch 76: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-0.0001)\n",
      "epoch 77: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-0.0001)\n",
      "epoch 78: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-9.2983e-05)\n",
      "epoch 79: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-7.8678e-05)\n",
      "epoch 80: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-6.6340e-05)\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-5.5254e-05)\n",
      "epoch 82: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-4.6849e-05)\n",
      "epoch 83: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-3.8981e-05)\n",
      "epoch 84: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-3.3796e-05)\n",
      "epoch 85: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-2.8610e-05)\n",
      "epoch 86: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-2.4676e-05)\n",
      "epoch 87: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-2.1458e-05)\n",
      "epoch 88: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-1.8239e-05)\n",
      "epoch 89: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-1.4305e-05)\n",
      "epoch 90: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-1.2338e-05)\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-1.0371e-05)\n",
      "epoch 92: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-9.1195e-06)\n",
      "epoch 93: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-7.1526e-06)\n",
      "epoch 94: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-5.1856e-06)\n",
      "epoch 95: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-5.1856e-06)\n",
      "epoch 96: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-5.1856e-06)\n",
      "epoch 97: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-5.1856e-06)\n",
      "epoch 98: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-5.1856e-06)\n",
      "epoch 99: w = 2.000, loss = 0.00000000\n",
      "w.grad tensor(-5.1856e-06)\n",
      "epoch 100: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Linear regression\n",
    "# f = w * x \n",
    "# here : f = 2 * x\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model output\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5).item():.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    # dw = gradient(X, Y, y_pred)\n",
    "    l.backward()\n",
    "    print(\"w.grad\",w.grad) # dl/dw\n",
    "\n",
    "    # update weights\n",
    "    # w.data = w.data - learning_rate * w.grad\n",
    "    # 이 내부에서 새로 생성된 텐서들은 requires_grad=False 상태가 되어, 메모리 사용량을 아껴준다.\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "    \n",
    "    # zero the gradients after updating\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "\n",
    "        print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5).item():.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0000, requires_grad=True)\n",
      "1.9999996423721313\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(w.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# 1) Design model (input, output, forward pass with different layers)\n",
    "# 2) Construct loss and optimizer\n",
    "# 3) Training loop\n",
    "#       - Forward = compute prediction and loss\n",
    "#       - Backward = compute gradients\n",
    "#       - Update weights\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Linear regression\n",
    "# f = w * x \n",
    "\n",
    "# here : f = 2 * x\n",
    "\n",
    "# 0) Training samples\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "# 1) Design Model: Weights to optimize and forward function\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5).item():.3f}')\n",
    "\n",
    "# 2) Define loss and optimizer\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "# callable function\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_predicted = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_predicted)\n",
    "\n",
    "    # 1. calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # 2. update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5).item():.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n",
      "#samples: 4, #features: 1\n",
      "self.lin Linear(in_features=1, out_features=1, bias=True)\n",
      "model LinearRegression(\n",
      "  (lin): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      ">>> tensor([4.1245], grad_fn=<AddBackward0>)\n",
      "Prediction before training: f(5) = 4.124\n",
      "w Parameter containing:\n",
      "tensor([[0.9427]], requires_grad=True)\n",
      "b Parameter containing:\n",
      "tensor([0.3218], requires_grad=True)\n",
      "epoch 1: w = 0.943, loss = 9.75332451\n",
      "w Parameter containing:\n",
      "tensor([[1.6618]], requires_grad=True)\n",
      "b Parameter containing:\n",
      "tensor([0.5462], requires_grad=True)\n",
      "epoch 11: w = 1.662, loss = 0.30974153\n",
      "w Parameter containing:\n",
      "tensor([[1.7824]], requires_grad=True)\n",
      "b Parameter containing:\n",
      "tensor([0.5677], requires_grad=True)\n",
      "epoch 21: w = 1.782, loss = 0.06207225\n",
      "w Parameter containing:\n",
      "tensor([[1.8066]], requires_grad=True)\n",
      "b Parameter containing:\n",
      "tensor([0.5570], requires_grad=True)\n",
      "epoch 31: w = 1.807, loss = 0.05251795\n",
      "w Parameter containing:\n",
      "tensor([[1.8152]], requires_grad=True)\n",
      "b Parameter containing:\n",
      "tensor([0.5415], requires_grad=True)\n",
      "epoch 41: w = 1.815, loss = 0.04930745\n",
      "w Parameter containing:\n",
      "tensor([[1.8211]], requires_grad=True)\n",
      "b Parameter containing:\n",
      "tensor([0.5257], requires_grad=True)\n",
      "epoch 51: w = 1.821, loss = 0.04643354\n",
      "w Parameter containing:\n",
      "tensor([[1.8265]], requires_grad=True)\n",
      "b Parameter containing:\n",
      "tensor([0.5102], requires_grad=True)\n",
      "epoch 61: w = 1.826, loss = 0.04373075\n",
      "w Parameter containing:\n",
      "tensor([[1.8316]], requires_grad=True)\n",
      "b Parameter containing:\n",
      "tensor([0.4951], requires_grad=True)\n",
      "epoch 71: w = 1.832, loss = 0.04118538\n",
      "w Parameter containing:\n",
      "tensor([[1.8366]], requires_grad=True)\n",
      "b Parameter containing:\n",
      "tensor([0.4805], requires_grad=True)\n",
      "epoch 81: w = 1.837, loss = 0.03878816\n",
      "w Parameter containing:\n",
      "tensor([[1.8414]], requires_grad=True)\n",
      "b Parameter containing:\n",
      "tensor([0.4663], requires_grad=True)\n",
      "epoch 91: w = 1.841, loss = 0.03653052\n",
      "Prediction after training: f(5) = 9.682\n",
      ">>> tensor([9.6820], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 1) Design model (input, output, forward pass with different layers)\n",
    "# 2) Construct loss and optimizer\n",
    "# 3) Training loop\n",
    "#       - Forward = compute prediction and loss\n",
    "#       - Backward = compute gradients\n",
    "#       - Update weights\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Linear regression\n",
    "# f = w * x \n",
    "\n",
    "# here : f = 2 * x\n",
    "\n",
    "# 0) Training samples, watch the shape!\n",
    "# X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "# Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(X.shape) # torch.Size([4, 1])\n",
    "print(f'#samples: {n_samples}, #features: {n_features}')\n",
    "#samples: 4, #features: 1\n",
    "\n",
    "# 0) create a test sample\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "# 1) Design Model, the model has to implement the forward pass!\n",
    "# Here we can use a built-in model from PyTorch\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "# we can call this model with samples X\n",
    "#model = nn.Linear(input_size, output_size)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define diferent layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "        print(\"self.lin\",self.lin)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(\"forward!\")\n",
    "        return self.lin(x)\n",
    "\n",
    "model = LinearRegression(input_size, output_size)\n",
    "print(\"model\",model)\n",
    "print(\">>>\", model(X_test))\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "# 2) Define loss and optimizer\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass with our model\n",
    "    y_predicted = model(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_predicted)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0:     \n",
    "        [w, b] = model.parameters() # unpack parameters\n",
    "        print(\"w\",w)\n",
    "        print(\"b\",b)\n",
    "        print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')\n",
    "print(\">>>\", model(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1) (100,)\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100])\n",
      "100\n",
      "torch.Size([100, 1])\n",
      "epoch: 10, loss = ++++++4119.6753\n",
      "epoch: 20, loss = ++++++2902.3110\n",
      "epoch: 30, loss = ++++++2072.2920\n",
      "epoch: 40, loss = ++++++1506.2522\n",
      "epoch: 50, loss = ++++++1120.1570\n",
      "epoch: 60, loss = +++++++856.7488\n",
      "epoch: 70, loss = +++++++677.0066\n",
      "epoch: 80, loss = +++++++554.3323\n",
      "epoch: 90, loss = +++++++470.5912\n",
      "epoch: 100, loss = +++++++413.4166\n",
      "(100, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 0) Prepare data\n",
    "X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=4)\n",
    "print(X_numpy.shape, y_numpy.shape) # (100, 1) (100,)\n",
    "# cast to float Tensor\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "print(X.shape) # torch.Size([100, 1])\n",
    "print(y.shape) # torch.Size([100])\n",
    "print(y.shape[0]) # 100\n",
    "# 텐서의 모양을 변경하려면 torch.reshape를 사용\n",
    "# 메모리 사용량도 염려하고 두 텐서가 동일한 데이터를 공유하도록 하려면 torch.view를 사용\n",
    "y = y.view(y.shape[0], 1)\n",
    "print(y.shape) # torch.Size([100, 1])\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# 1) Model\n",
    "# Linear model f = wx + b\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# 2) Loss and optimizer\n",
    "learning_rate = 0.01\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# 3) Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass and loss\n",
    "    y_predicted = model(X)\n",
    "    loss = criterion(y_predicted, y)\n",
    "    \n",
    "    # Backward pass and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero grad before new step\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():+>15.4f}')\n",
    "\n",
    "# Plot\n",
    "predicted = model(X).detach().numpy()\n",
    "#predicted = model(X).clone().numpy() ->  RuntimeError: Can't call numpy() on Tensor that requires grad.\n",
    "print(predicted.shape)\n",
    "\n",
    "plt.plot(X_numpy, y_numpy, 'ro')\n",
    "plt.plot(X_numpy, predicted, 'b')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(455, 30)\n",
      "(455,)\n",
      "torch.Size([455, 30])\n",
      "torch.Size([455])\n",
      "torch.Size([114, 30])\n",
      "torch.Size([114])\n",
      "torch.Size([455, 1])\n",
      "torch.Size([114, 1])\n",
      "* torch.Size([114, 30])\n",
      "torch.Size([114, 1])\n",
      "tensor([[ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False]])\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1])\n",
      "epoch: 10, loss = 0.6183\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1])\n",
      "epoch: 20, loss = 0.5142\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1])\n",
      "epoch: 30, loss = 0.4468\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1])\n",
      "epoch: 40, loss = 0.3995\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1])\n",
      "epoch: 50, loss = 0.3643\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1])\n",
      "epoch: 60, loss = 0.3369\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1])\n",
      "epoch: 70, loss = 0.3148\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1])\n",
      "epoch: 80, loss = 0.2965\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1])\n",
      "epoch: 90, loss = 0.2810\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1])\n",
      "epoch: 100, loss = 0.2677\n",
      "torch.Size([114, 30])\n",
      "torch.Size([114, 1])\n",
      "tensor([[ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True]])\n",
      "accuracy: 0.9123\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 0) Prepare data\n",
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(X.shape) # (569, 30) -> (455,30) + (114,30)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "# scale\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "print(X_train.shape) # (455, 30)\n",
    "print(y_train.shape) # (455,)\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "print(X_train.shape) # torch.Size([455, 30])\n",
    "print(y_train.shape) # torch.Size([455])\n",
    "print(X_test.shape)  # torch.Size([114, 30])\n",
    "print(y_test.shape)  # torch.Size([114])\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "print(y_train.shape) # torch.Size([455, 1])\n",
    "print(y_test.shape)  # torch.Size([114, 1])\n",
    "\n",
    "# 1) Model\n",
    "# Linear model f = wx + b , sigmoid at the end\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1) # (30, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "model = Model(n_features)\n",
    "y_predicted = model(X_test)\n",
    "print(X_test.shape)      # torch.Size([114, 30])\n",
    "print(y_predicted.shape) # torch.Size([114, 1])\n",
    "y_predicted_cls = y_predicted.round()\n",
    "print(y_predicted_cls.eq(y_test))\n",
    "\n",
    "# 2) Loss and optimizer\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss() # Binary Cross Entropy\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass and loss\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    # Backward pass and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero grad before new step\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        [w, b] = model.parameters() # unpack parameters\n",
    "        print(w.shape) # torch.Size([1, 30])\n",
    "        print(b.shape) # torch.Size([1])\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "# evaluate 할 때는 가중치가 업데이트가 되어서는 안된다. \n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    print(X_test.shape)      # torch.Size([114, 30])\n",
    "    print(y_predicted.shape) # torch.Size([114, 1])\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    print(y_predicted_cls.eq(y_test))\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy: {acc.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 14)\n",
      "torch.Size([13]) torch.Size([1])\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000002311ACE8548>\n",
      "torch.Size([4, 13]) torch.Size([4, 1])\n",
      "178\n",
      "178 45\n",
      "Epoch: 1/2, Step 5/45 | Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 1/2, Step 10/45 | Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 1/2, Step 15/45 | Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 1/2, Step 20/45 | Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 1/2, Step 25/45 | Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 1/2, Step 30/45 | Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 1/2, Step 35/45 | Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 1/2, Step 40/45 | Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 1/2, Step 45/45 | Inputs torch.Size([2, 13]) | Labels torch.Size([2, 1])\n",
      "Epoch: 2/2, Step 5/45 | Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 2/2, Step 10/45 | Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 2/2, Step 15/45 | Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 2/2, Step 20/45 | Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 2/2, Step 25/45 | Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 2/2, Step 30/45 | Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 2/2, Step 35/45 | Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 2/2, Step 40/45 | Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 2/2, Step 45/45 | Inputs torch.Size([2, 13]) | Labels torch.Size([2, 1])\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c81a1cf24b40568d3a5ab3faa44968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2cb7d2c98824edbab3cdfc68fc00b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab187d37fe04f3ebcca3dc63552b85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37742352edd4275870b0a4c90aa1608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "torch.Size([3, 1, 28, 28]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# gradient computation etc. not efficient for whole data set\n",
    "# -> divide dataset into small batches\n",
    "\n",
    "'''\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # loop over all batches\n",
    "    for i in range(total_batches):\n",
    "        batch_x, batch_y = ...\n",
    "'''\n",
    "\n",
    "# epoch = one forward and backward pass of ALL training samples\n",
    "# batch_size = number of training samples used in one forward/backward pass\n",
    "# number of iterations = number of passes, each pass (forward+backward) using [batch_size] number of sampes\n",
    "# e.g : 100 samples, batch_size=20 -> 100/20=5 iterations for 1 epoch\n",
    "\n",
    "# --> DataLoader can do the batch computation for us\n",
    "\n",
    "# Implement a custom Dataset:\n",
    "# inherit Dataset\n",
    "# implement __init__ , __getitem__ , and __len__\n",
    "\n",
    "class WineDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize data, download, etc.\n",
    "        # read with numpy or pandas\n",
    "        xy = np.loadtxt('./data/wine/wine.csv', delimiter=',', dtype=np.float32, skiprows=1) # skiprows=1 -> header\n",
    "        print(xy.shape)              # (178, 14)\n",
    "        self.n_samples = xy.shape[0] # 178\n",
    "\n",
    "        # here the first column is the class label, the rest are the features\n",
    "        self.x_data = torch.from_numpy(xy[:, 1:]) # size [n_samples, n_features]\n",
    "        self.y_data = torch.from_numpy(xy[:, [0]]) # size [n_samples, 1] -> 열 벡터로 만들기 위함\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "# create dataset\n",
    "dataset = WineDataset()\n",
    "# get first sample and unpack\n",
    "first_data = dataset[0] # csv파일 첫행을 읽어 온다\n",
    "features, labels = first_data\n",
    "print(features.shape, labels.shape) # torch.Size([13]) torch.Size([1])\n",
    "\n",
    "# Load whole dataset with DataLoader\n",
    "# shuffle: shuffle data, good for training\n",
    "# num_workers: faster loading with multiple subprocesses\n",
    "# !!! IF YOU GET AN ERROR DURING LOADING, SET num_workers TO 0 !!!\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=4,\n",
    "                          shuffle=True, # False하면 배치사이즈만큼 1행부터 순서대로 가져온다\n",
    "                          num_workers=0)\n",
    "print(train_loader) # <torch.utils.data.dataloader.DataLoader object>\n",
    "\n",
    "# convert to an iterator and look at one random sample\n",
    "dataiter = iter(train_loader)\n",
    "data = dataiter.next()\n",
    "features, labels = data\n",
    "print(features.shape, labels.shape) # torch.Size([4, 13]) torch.Size([4, 1])\n",
    "\n",
    "# Dummy Training loop\n",
    "num_epochs = 2\n",
    "total_samples = len(dataset)\n",
    "print(total_samples) # 178\n",
    "n_iterations = math.ceil(total_samples/4) # 178/4 = 44.5 -> 45                     \n",
    "print(total_samples, n_iterations)        # 178, 45\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        \n",
    "        # here: 178 samples, batch_size = 4, n_iters=178/4=44.5 -> 45 iterations\n",
    "        # 4 x 44 + 2 x 1 -> 178 samples\n",
    "        # Run your training process\n",
    "        if (i+1) % 5 == 0:\n",
    "            print(f'Epoch: {epoch+1}/{num_epochs}, Step {i+1}/{n_iterations} | Inputs {inputs.shape} | Labels {labels.shape}')\n",
    "\n",
    "# some famous datasets are available in torchvision.datasets\n",
    "# e.g. MNIST, Fashion-MNIST, CIFAR10, COCO\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=torchvision.transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=3, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# look at one random sample\n",
    "dataiter = iter(train_loader)\n",
    "data = dataiter.next()\n",
    "inputs, targets = data\n",
    "print(inputs.shape, targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
